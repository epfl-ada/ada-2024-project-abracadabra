{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils_functions\n",
    "reload(utils_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Understanding and treating the data\n",
    "\n",
    "## Part 1.1 Data conversion\n",
    "\n",
    "The first step in our analysis is to ensure all data is represented consistently across the project. This involves converting the original **.txt** files containing ratings and reviews into **.csv** format. Each file was examined carefully, the strings were stored into dictionaries representing key data fields. More details on the conversion process and methodology can be found in **data/TxtToCsv.ipynb**\n",
    "\n",
    "The CSV files can be found on the following link: https://drive.google.com/drive/folders/1lcRRxlPpcyAcqJzanlwcyb5Vmip0s7_D?usp=sharing\n",
    "(You will need to ask for permission to see the files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 Data exploration\n",
    "\n",
    "With the data now in a consistent format, we begin exploring deeper the datasets to understand their links and features. We learn especially that some breweries, beers, users matches between the websites. Moreover, their might be duplicates within the datasets, with some users having multiple accounts. Breweries also have duplicates: i.e a single brewery in ratebeer can correspond to up to 3 breweries in advocate. The goal of this notebook was also to look at the percentage of Nan values for the ratings, and to understand the different variables. Further explanations can be found in **data/data_understanding.ipynb**.\n",
    "\n",
    "Furthermore an other Jupyter Notebook explores Nan values in the dataset. It is mainly a secondary file that was used to evaluate the percentage of Nan in columns of a dataframe. In there we look at the min/max value of the different grades, where we noticed that both datasets not necessarily contain the same range for their grades. More information can be found in **data/data_cleaning.ipynb**.\n",
    "\n",
    "Some rows contain NaN values in the datasets. Since certain parts of the analysis do not require every feature, we handle missing data filtering based on the requirements of each analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3 Dataset merging\n",
    "To enhance the robustness of the analysis, we merge data from both RateBeer and BeerAdvocate. This approach increases the number of ratings per beer enabling a higher reliability and enhancing the controversiality analysis.\n",
    "\n",
    "The aim is to have a general dataset for users, beers, breweries and ratings. For users, breweries, ratings each one contains a new id, the old id from both datasets except for ratings. It also contains the name, location and other information. If it is match we often make a choice from which dataset to use the information, for example as two breweries in advocate are a single in ratebeer we decided to take the name of the brewery from ratebeer. We filter out the matched informations as a single one.\n",
    "\n",
    "For ratings we have every rating of both datasets, even the one matched twice. We decided to keep them both as we noticed that comments may differ in the grading and textual description. We added columns corresponding to the new beer, user and brewery id. Certain attributes where deleted as we assumed that we could recompute them again, or if need could reload the old files. Finally we gave an id to each rating, the dataset of origin as 'rb' or 'ad', and we added a column called matched if filled with a number contains the id of the rating it is matched with.\n",
    "\n",
    "Further information can be found add **data/merge_into_onedataset.ipynb**. The transformed data can be found at the following link:\n",
    "https://drive.google.com/drive/folders/1McQ7BU24mEsEqouulOPqrmtQJ47E6ZP8?usp=sharing\n",
    "(You will need to ask for permission).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "\n",
    "For the whole the next cell calls the different datasets and is used for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.getcwd()).parent\n",
    "\n",
    "#Change for each one of where your data is. For me in Dataset I have all the three folders\n",
    "dataset_path = os.path.join(root,'Dataset')\n",
    "\n",
    "FULL = \"full\"\n",
    "FULL_PATH = os.path.join(dataset_path,FULL)\n",
    "\n",
    "breweries_df = pd.read_csv(os.path.join(FULL_PATH, 'breweries.csv'))\n",
    "beers_df = pd.read_csv(os.path.join(FULL_PATH, 'beers.csv'))\n",
    "users_df = pd.read_csv(os.path.join(FULL_PATH, 'users.csv'))\n",
    "ratings_df = pd.read_csv(os.path.join(FULL_PATH,'ratings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grade transformation**\n",
    "\n",
    "We noticed during the **data_cleaning.ipynb** that the grades are not based on the same range. We decided to set the grade between 1 and 5. If the attribute is between 1 and 20 and the grade is set at 16/20 it will become (16-1)/19*4+1 = 4.15 and not 4. 1 comes from the min value of the attribute, and 19 because of the span. We decided to set it between 1 and 5 as we noticed that most attributes are graded with this range of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = recompute_grade(ratings_df, min_grade_value = 1, max_grade_value = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Exploring the definition of controversiality\n",
    "\n",
    "This part aims to determine and label which beers are controversial and which are universal. To do this, we explore different aspects defining the controversiality of a beer. \n",
    "\n",
    "What does controversial mean : \"subject of intense public argument, disagreement, or disapproval\" [1]. As described, this depends on the opinions of the people. As a result, this analysis only depends on the fields the users can fulfill, namely, the different ratings : appearance, aroma, palate, taste, overall, and the textual reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 Ratings and reviews filtering\n",
    "\n",
    "As mentioned earlier, controversiality depends on disagreement in opinions. Beers with few ratings are more likely to show high variability (e.g. two opposing opinions). To ensure reliable insights and meaningful analysis, we exclude beers with fewer number of ratings or reviews than a specified threshold. Later, we might apply a weighting factor based on rating count to further refine the controversiality analysis, according more importance to more rated beers.\n",
    "\n",
    "The threshold deciding wheter to keep a beer in the analysis is chosen at 10 arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Part 2.2, we want to have a DataFrame with the relevant features : id_beer, appearance, aroma, taste, palate, overall, ratings.\n",
    "# We only keep the beer having enough ratings for a controversial analysis\n",
    "beer_ratings = filter_ratings(ratings_df, threshold=10, attributes=['appearance', 'aroma', 'palate', 'taste', 'overall'])\n",
    "\n",
    "# For Part 2.3, we want to have a DataFrame with the relevant features : id_beer, text. \n",
    "# We only keep the beer having enough reviews for a controversial analysis\n",
    "beer_reviews = filter_ratings(ratings_df, threshold=10, attributes=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_ratings.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_reviews.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Beer attributes controversiality analysis\n",
    "\n",
    "Controversiality can be analyzed in different manners. For now, the three following definitions are studied :\n",
    "\n",
    "- We compute the variance of each attributes (appearance, aroma, palate, taste) for each beer, then study which attribute seems to be the most controversial by looking at the distribution of the variances.\n",
    "- Knowing **overall** is the global grade given by the user, we use it to classify the beers. We compute the variance across the **overall** rating. We then classify beers as controversial/universal accoring to a certain threshold. Then, we observe which of the four attributes influences the most the overall score controversiality by looking at the distribution of the variances in each classes.\n",
    "- We define for each beer which attributes is the most and least controversial looking at its variance. Then we count the number of max/min variances for all attributes. This can also be done after classification with **overall** score.\n",
    "\n",
    "BIEN DIRE A LA FIN DE LA PART 2 QU'ON AIMERAIT AU FINAL CHOPPER UN MIX DU SENIMENTAL ANALYSIS ET D'UNE DES VARIANTES RELEVANTE QUI UTILISE LES ATTRIBUTS POUR CLASSIFIER LES BIERES COMME CONTROVERSIAL OU UNIVERSAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.1 Features analysis controversiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance of each attribute on each beer\n",
    "attributes_variance = compute_variance_per_attribute(beer_ratings, ['appearance', 'aroma', 'palate', 'taste','overall'])\n",
    "attributes_variance.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_variance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_distrib_violin(attributes_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have many outliers. Palate and appearance seem to have the highest variance STILL TO COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.2 Features analysis from overall controversiality\n",
    "\n",
    "In this part, we want to try analyzing the attributes using the variance of the overall feature. We classify a beer as controversial/universal according to overall variance and then study the variance of the attributes within these class. This can be done by :\n",
    "1) Defining value threshold, granting greater choice freedom\n",
    "2) Selecting the highest and lowest pourcentage of the distribution of variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of variance across the beers for overall attributes\n",
    "plot_overall_hist_distrib(attributes_variance['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Compute the variance of the 4 attributes according to the classification using overall variance score\n",
    "# Threshold are decided to be 0.05 universal, 0.5 controversial, arbitrarily\n",
    "[controv_attrib_var, univ_attrib_var] = classify_value_threshold(ratings_df, ['appearance', 'aroma', 'palate', 'taste'], ['overall'], 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controv_attrib_var.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_attrib_var.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) # Compute the variance of the 4 attributes according to the classification using overall variance score\n",
    "# Threshold is decided to be 10%, arbitrarily\n",
    "[controv_attrib_var, univ_attrib_var] = classify_percentage_distribution(beer_ratings, ['appearance', 'aroma', 'palate', 'taste'], ['overall'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controv_attrib_var.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. We learn that when a beer has a controversial overall score, palate has a higher expected variance. It is followed by taste, appearance and finally aroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_attrib_var.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a beer has a universall overal score, appearance and palate have really high expected variance, while taste and aroma have a lower variance.\n",
    "\n",
    "Looking closer at taste attribute, we see that when overall variance is high, taste variance is high. If overall variance is low, taste variance is low. As a result, taste might be the main responsible of the overall variance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_boxplot(controv_attrib_var, univ_attrib_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results seems to indicate that when there is strong disagreement on a beer, highest reason of this disagreement might stem from taste and palate attributes. When a beer is labeled as universal, it seems that appearance and palate remain highest versatile scores, while aroma and taste are low as well.\n",
    "\n",
    "As expected, attributes variances are positively correlated with overall variance. However, taste seems to be the most correlated one. Thus, most of the overall grade might be explained by this attribute\n",
    "\n",
    "REPLACE ???????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.3 Feature analysis from count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of max min variance attributes\n",
    "[max_var_count, min_var_count] = max_min_variance_count(attributes_variance.drop(['overall'], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_max_min_variance_count(max_var_count, min_var_count, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we observe that palate and appearance are the attributes having the maximal variance for most of the beers, while aroma and taste mostly have the minimal variance.\n",
    "\n",
    "Let's try to do the same analysis but using the classification done in part 2.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[max_var_count, min_var_count] = max_min_variance_count(controv_attrib_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_max_min_variance_count(max_var_count, min_var_count, 'controversial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[max_var_count, min_var_count] = max_min_variance_count(univ_attrib_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_max_min_variance_count(max_var_count, min_var_count, 'universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.4 Correlation between the variance of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of the attribute's variances with overall variance\n",
    "correlations = compute_correlation(attributes_variance, ['overall'])\n",
    "plot_correlation(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.5 T test analysis on the varaiance of the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to test if the variance of the different attributes have a similar mean. For this we perform a T Test between the variance of each attribute, where the H0 hypothesis is that the true mean of the variance of a given attribute between the different bears are equal. H1 is that the means are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_statistic(attributes_variance, attributes_of_interest = ['appearance', 'aroma', 'palate', 'taste','overall','rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a significance level of 0.05, we can reject H0 hypothesis every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.6 PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_plot(attributes_variance,attributes_of_interest_PCA= ['appearance', 'aroma', 'palate', 'taste','overall','rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we see that the density is quite high at the center of the plot. We cannot separate easily the data in two from the PCA graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3: Analysis of the Reviews\n",
    "\n",
    "To better understand the meaning behind the ratings we previously examined, we will conduct two types of analyses: sentiment analysis and semantic similarity analysis between relevant topics and the reviews.\n",
    "\n",
    "### Part 2.3.1: Sentiment Analysis\n",
    "\n",
    "We will begin by performing a sentiment analysis on the reviews. This will help us gain deeper insights when we later classify the reviews by topics. The primary objective is to identify a reliable, multilingual model. To achieve this, we will compare the performance of various models:  A [BERT base multilingual uncased model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment), [Google Cloud NLP](https://cloud.google.com/natural-language/docs/analyzing-sentiment?hl=fr), [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) and a \n",
    "[distilbert base multilingual cased model](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student).\n",
    "\n",
    "We will compare the performance of each model on the first 6,000 reviews. We chose 6,000 reviews because it is a large enough sample for a minimally biased analysis while helping save on API credits.\n",
    "\n",
    "Each review was labeled as Positive (1), Neutral (0), or Negative (-1).  We kept the result of this analysis in the _sentiment_analysis.csv_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_functions import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv(\"src/scripts/sentiment_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compared the models’ performances based on exact similarity and similarity within a range of ±1. The latter is particularly relevant because it allows us to identify models that may produce false positives or false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, exact_similarity_columns, plus_minus_1_similarity_columns] = compute_similarity_scores(df_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to plot the result in the following bar plots for the exact and ±1 similarity scores across different pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_similarities(df, exact_similarity_columns, plus_minus_1_similarity_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, looking at the bar plots, the BERT and Distlilbert, the Google Cloud NLP and GPT-4o mini models, as well as the BERT anf Google Cloud NLP models show the highest similarity for exact sentiment, exceeding 65%.\n",
    "\n",
    "Additionally, the Google Cloud NLP and GPT-4o mini models have nearly 100% similarity when considering a margin, followed closely by the BERT-based model and GPT-4o mini, and then by the DistilBERT model and GPT-4o mini.\n",
    "\n",
    "Given that GPT-4o mini, the BERT model and Google Cloud NLP are very similar in the second graph, that GPT-4o mini is falling behind in the exact sentiment comparison and the DistilBERT model is behind the other models in both comparisons, we should use either Google Cloud NLP or the BERT-based model. Even though Google Cloud NLP has a slightly better similarity in both comparisons, it also an expensive api for large datasets. We will therefore use the BERT-based model for the final sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3.2: Semantic Similarity Analysis\n",
    "\n",
    "The next step is to identify relevant themes in the comments, such as flavor, color, and others. This, combined with sentiment analysis and an LDA model, will help us better understand why certain characteristics might be controversial.\n",
    "\n",
    "To achieve this, we will first use an embedding model on both the reviews and the themes, and apply cosine similarity (see function below) to determine which comments are \"close\" to each theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    :param vec_a: Review vector.\n",
    "    :param vec_b: Theme vector.\n",
    "    :return: Cosine similarity.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use an LDA model to generate topics that may be relevant to each theme.\n",
    "\n",
    "\n",
    "The next steps are as follows :\n",
    "* Assess and demonstrate the reliability of the selected embedding model.\n",
    "* Evaluate and demonstrate the reliability of the chosen LDA model.\n",
    "* For the controversial beers/themes, identify the most recurring topics in the reviews.\n",
    "* If performance is lacking, we could consider using a summarizer as an alternative to LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Which beer is controversial then ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue our controversiality analysis we now need to determine which beer can be deemed controversial and which are universal.\n",
    "\n",
    "We decided to compute a _controversiality score_ using the results from both our analysis on the feature variance and the sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4.1 Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the previously mentionned _controversiality score_ to label the whole dataset. This will later allows us to determine which parameter of the beer are responsible for the controversiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4.2 Statistical testing and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will perfom statistical tests to validate our _controversiality score_.\n",
    "\n",
    "We will first use hypothesis testing to determine if the differences over the feature variances and sentiment analysis are statistically significant. We will compare the distribution of those metrics for controversial and universal beers.\n",
    "\n",
    "Then we will apply cross-validation, by splitting the dataset into train and validation to assert the generalisation of our score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Some reasons of controversiality\n",
    "\n",
    "This part uses the controversiality label attributed to the beers. It tries to find patterns and reasons of controversial opinion as a function of an inherent variables of the beers: such as abv and style of the beer, location of the brewery and the users, level of expertise of the users..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1 Expertise analysis\n",
    "\n",
    "In this part, we classify the users related to how many ratings they did.\n",
    "\n",
    "- Novice are users with only a few ratings : 1-20.\n",
    "- Enthusiasts are users with moderate number of ratings : 21-100\n",
    "- Connoisseur are users with high nuber of ratings . 101+\n",
    "\n",
    "It is important to note that this choice has been arbitrarily made. It could be made differently or could be interactive for the reader of the story, enabling him to label users differently according to how many ratings he thinks is enough to be a connoisseur/enthusiasts/novice.\n",
    "\n",
    "Another essential thing to take into account is that these classes do not represent users as novice or connoisseur about **beers**, but about **rating** on each particular website.\n",
    "\n",
    "First step is to classify the users in the three mentioned categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = classify_user_rating_level(users_df, enthusiasts_level=21, connoisseur_level=101)\n",
    "plot_category_distrib(user_df, 'rating_user_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2 Style of the beer and abv\n",
    "\n",
    "In this part, we investigate the hypothesis that certain types of beers are more controversial than others.\n",
    "\n",
    "To do so we will group the beers by style and ABV, analyzing the distribution of controversial ratings within these categories. Statistical tests will be performed to check whether controversiality is significantly associated with either of these beer characteristics. Following this, we will examine whether there are significant differences in the distribution of styles or ABV levels between controversial and non-controversial beers.\n",
    "\n",
    "Finally, it will be interesting to explore specific combinations of style and ABV that may contribute to a beer's controversiality. These informations could help identify the types of beers that are more polarizing among consumers, potentially revealing taste characteristics or attributes that lead to divergent opinions."

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3 Patterns in location and ratings of local or foreign beers\n",
    "\n",
    "In this part, we analyse the geographic factor on the controversiality of a beer. The main interest is to investigate for any local influence on the controversiality of a beer.\n",
    "\n",
    "Using sentiment analysis on comments, we aim to investigate the relationship between the location of a beer and the location of the reviewer by examining:\n",
    "\n",
    "<ul>\n",
    "  <li>How is a user’s opinion influenced if the beer originates from their own region? Are reviewers more critical for local or foreign beers?</li>\n",
    "  <li>Are there specific regions where users are generally more selective or strict (for local or foreign beers)?</li>\n",
    "  <li>Are there certain combinations of user and beer regions that lead to higher controversiality?</li>\n",
    "  <li>Does the importance of a particular beer style in a region influence the controversiality of other beer styles?</li>\n",
    "</ul>\n",
    "\n",
    "In addition, we will assess whether some regions show a stronger polarization (extreme positive or negative sentiments) in their ratings (overall and detailed), and we will analyze if such polarization correlates with controversiality. Similarly, we will analyse if there are combinations of regions and beer characteristic that leads to higher controversiality.\n",
    "\n",
    "Finally, with all these data, we will try to define the \"perfect\" beer for each region: the one which is the less controversial. It will also be interesting to see if there are regional specificities or if beers enjoyers tends to agree worldwide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3 Patterns in location and ratings of local or foreign beers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] : “Definition of Controversial.” *Collinsdictionary.com*, HarperCollins Publishers Ltd, 14 Oct. 2024, www.collinsdictionary.com/dictionary/english/controversial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
