{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Some Beers Spark Debate: A Short Walk Through Controversial Brews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### 1. Understanding and treating the data (finished)\n",
    "- **1.1 Data conversion**\n",
    "- **1.2 Data exploration**\n",
    "- **1.3 Dataset Merging**\n",
    "### 2. Exploring the definition of controversiality (in construction)\n",
    "- **2.1 Ratings and reviews filtering** (in construction)\n",
    "- **2.? Clustering the beers** (finished)\n",
    "### 3. Extracting information from the textual reviews (just validate pls)\n",
    "- **3.1 Sentiment analyses** (finished, but Gustave check pls)\n",
    "- **3.2 Comparing the distribution with the other attributes** (finished)\n",
    "- **3.3 Clustering and comparison against the previous clusters** (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import src.utils.utils_functions\n",
    "reload(src.utils.utils_functions)\n",
    "from src.utils.utils_functions import *\n",
    "\n",
    "import src.utils.clustering_GMM\n",
    "reload(src.utils.clustering_GMM)\n",
    "from src.utils.clustering_GMM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from src.utils.utils_functions import *\n",
    "from src.utils.clustering_GMM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Understanding and treating the data\n",
    "\n",
    "## Part 1.1 Data conversion\n",
    "\n",
    "The first step in our analysis is to ensure all data is represented consistently across the project. This involves converting the original **.txt** files containing ratings and reviews into **.csv** format. Each file was examined carefully, the strings were stored into dictionaries representing key data fields. More details on the conversion process and methodology can be found in **data/TxtToCsv.ipynb**\n",
    "\n",
    "The CSV files can be found on the following link: https://drive.google.com/drive/folders/1lcRRxlPpcyAcqJzanlwcyb5Vmip0s7_D?usp=sharing\n",
    "(You will need to ask for permission to see the files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 Data exploration\n",
    "\n",
    "With the data now in a consistent format, we begin exploring deeper the datasets to understand their links and features. We learn especially that some breweries, beers, users matches between the websites. Moreover, their might be duplicates within the datasets, with some users having multiple accounts. Breweries also have duplicates: i.e a single brewery in ratebeer can correspond to up to 3 breweries in advocate. The goal of this notebook was also to look at the percentage of Nan values for the ratings, and to understand the different variables. Further explanations can be found in **data/data_understanding.ipynb**.\n",
    "\n",
    "Furthermore an other Jupyter Notebook explores Nan values in the dataset. It is mainly a secondary file that was used to evaluate the percentage of Nan in columns of a dataframe. In there we look at the min/max value of the different grades, where we noticed that both datasets not necessarily contain the same range for their grades. More information can be found in **data/data_cleaning.ipynb**.\n",
    "\n",
    "Some rows contain NaN values in the datasets. Since certain parts of the analysis do not require every feature, we handle missing data filtering based on the requirements of each analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3 Dataset merging\n",
    "To enhance the robustness of the analysis, we merge data from both RateBeer and BeerAdvocate. This approach increases the number of ratings per beer enabling a higher reliability and enhancing the controversiality analysis.\n",
    "\n",
    "The aim is to have a general dataset for users, beers, breweries and ratings. For users, breweries, ratings each one contains a new id, the old id from both datasets except for ratings. It also contains the name, location and other information. If it is match we often make a choice from which dataset to use the information, for example as two breweries in advocate are a single in ratebeer we decided to take the name of the brewery from ratebeer. We filter out the matched informations as a single one.\n",
    "\n",
    "For ratings we have every rating of both datasets, even the one matched twice. We decided to keep them both as we noticed that comments may differ in the grading and textual description. We added columns corresponding to the new beer, user and brewery id. Certain attributes where deleted as we assumed that we could recompute them again, or if need could reload the old files. Finally we gave an id to each rating, the dataset of origin as 'rb' or 'ad', and we added a column called matched if filled with a number contains the id of the rating it is matched with.\n",
    "\n",
    "Further information can be found add **data/merge_into_onedataset.ipynb**. The transformed data can be found at the following link:\n",
    "https://drive.google.com/drive/folders/1McQ7BU24mEsEqouulOPqrmtQJ47E6ZP8?usp=sharing\n",
    "(You will need to ask for permission).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "\n",
    "For the whole the next cell calls the different datasets and is used for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.getcwd()).parent\n",
    "\n",
    "# Files are located in Dataset folder\n",
    "dataset_path = os.path.join(root,'Dataset')\n",
    "\n",
    "FULL = \"full\"\n",
    "FULL_PATH = os.path.join(dataset_path,FULL)\n",
    "\n",
    "breweries_df = pd.read_csv(os.path.join(FULL_PATH, 'breweries.csv'))\n",
    "beers_df = pd.read_csv(os.path.join(FULL_PATH, 'beers.csv'))\n",
    "users_df = pd.read_csv(os.path.join(FULL_PATH, 'users.csv'))\n",
    "ratings_df = pd.read_csv(os.path.join(FULL_PATH,'ratings.csv'))\n",
    "sentiment_df = pd.read_csv(os.path.join(FULL_PATH,'beer_reviews_with_sent.csv'))\n",
    "sentiment_analysis_df = pd.read_csv(os.path.join(FULL_PATH,'sentiment_analysis.csv'))\n",
    "ratings_with_sent_df = ratings_df.join(sentiment_df.set_index('id'), on='id')\n",
    "del sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grade distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stastics(ratings_df) # FAUDRAIT MODIFIER CETTE \n",
    "# FONCTION POUR QU'ELLE PRINT AVEC DES RETOUR Ã  LA LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_distrib_violin_grades(ratings_df=ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grade transformation**\n",
    "We notice during the data cleaning that grades do not have the same range and resolution on both webistes. \n",
    "\n",
    "BeerAdvocate allow the user to rate five attributes (aroma, palate, taste, appearance, overall) on a scale of 1 to 5 with a resolution of 0.25, while RateBeer allow the user to rate aroma and taste between 1 and 10, appearance palate between 1 and 5 and overall between 1 and 20, all with a resolution of 1.\n",
    "\n",
    "To perform our analysis, we must transform the grades to have the same scale between the two dataset. We decided to set the grade between 1 and 5 as most attributes are graded with this range. This is done by converting RateBeer scales according to the following formula : \n",
    "\n",
    "$$ \\frac{\\text{grade} - \\text{min}}{\\text{max} - \\text{min}} \\cdot 4 + 1 $$\n",
    "- with $\\text{max}$ the largest grade one can give to a given attribute.\n",
    "- with $\\text{min}$ the lowest grade one can give to a given attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = recompute_grade(ratings_df, min_grade_value = 1, max_grade_value = 5, attributes_of_interest = ['appearance', 'aroma', 'palate', 'taste', 'overall', 'rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grade distribution after rescaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_distrib_violin_grades(ratings_df=ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it results in attributes having different resolutions. This might be a problem when computing some metrics to define the controversiality and comparing between attributes. This will be discussed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Exploring the definition of controversiality\n",
    "\n",
    "This part aims to determine and label which beers are controversial and which are universal. To do this, we explore different aspects defining the controversiality of a beer. \n",
    "\n",
    "What does controversial mean : \"subject of intense public argument, disagreement, or disapproval\" [1]. As described, this depends on the opinions of the people. As a result, this analysis only depends on the fields the users can fulfill, namely, the different ratings : appearance, aroma, palate, taste, overall, and the textual reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to label the beers using the grades provided by the user, later we will try to add the sentiment analysis and observe if it helps in separating the beers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 Ratings and reviews filtering\n",
    "\n",
    "As mentioned earlier, controversiality depends on disagreement in opinions. Beers with few ratings are more likely to show high variability (e.g. two opposing opinions). To ensure reliable insights and meaningful analysis, we exclude beers with fewer number of ratings or reviews than a specified threshold. (((((((((((Later, we might apply a weighting factor based on rating count to further refine the controversiality analysis, according more importance to more rated beers.)))))))))))\n",
    "\n",
    "Let's try to find a reasonable threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_threshold_filtering(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the threhsold is : x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_ratings, breweries_df, beers_df, users_df = filter_ratings_new(ratings_df, beers_df, breweries_df,users_df, threshold=10, attributes=['appearance', 'aroma', 'palate', 'taste', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I just need it for the rest of the code. So add it where you want, or where it will be used.\n",
    "attributes_variance = compute_variance_per_attribute(beer_ratings, ['appearance', 'aroma', 'palate', 'taste','overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.? Clustering the beers\n",
    "\n",
    "Through are previous analysis we did not find any easy threshold, therefore we decided to apply a Gaussian Mixture Model (GMM) to the variance of our beers. Let us first perform a GMM analysis from 1 to 10 clusters and look at the negative log-likelihood of our clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nll(attributes_variance, min = 1, max = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the elbow method the plot of the negative log likelihood, with a varying number of clusters, we see that the optimal number of clusters would we 3. This is perfect since, at the end we would like to have three clusters corresponding to universal, neutral and controversial. Now let's have a look at what the clusters are to attribute the various labels to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = visualize_gmm_all_dimensions(attributes_variance, n_components_gmm=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_label_statistics(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the clusters where computed on the variance scaled to 0 mean and 1 as a standard deviation. \n",
    "\n",
    "Our definition of controversiality is that a beer is controversial if it has a high various across ratings, and a universal beer is a beer which has low variance across its ratings. The values appearing on the plot are not the real ones, just the scaled variances. As our variances are positive, we assume that the smallest \"scaled variances\" are the ones that where near 0, and the bigger ones are the ones with a higher variance. So by using the definition we can define the clusters as the following:\n",
    "- 0 : neutral, which represents 55.42% of the beers with a sufficient amount of ratings\n",
    "- 1 : controversial, which represents 23.11% of the beers with a sufficient amount of ratings\n",
    "- 2 : universal, which represents 21.47% of the beers with a sufficient amount of ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Extracting information from the textual reviews\n",
    "\n",
    "In the previous part we used almost every aspect that the rater could grade. However we did not use the textual analyses provided by the graders. In this part we aim at choosing a model to perform sentiment analyses, and to observe if the use of the score of the sentiment analyses can further enhance the separation of the different beers into controversiality/universality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1: Sentiment Analysis\n",
    "\n",
    "We will begin by performing a sentiment analysis on the reviews. This will help us gain deeper insights when we later classify the reviews by topics. The primary objective is to identify a reliable, multilingual model. To achieve this, we will compare the performance of various models:  A [BERT base multilingual uncased model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment), [Google Cloud NLP](https://cloud.google.com/natural-language/docs/analyzing-sentiment?hl=fr), [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) and a \n",
    "[distilbert base multilingual cased model](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student).\n",
    "\n",
    "We will compare the performance of each model on the first 6,000 reviews. We chose 6,000 reviews because it is a large enough sample for a minimally biased analysis while helping save on API credits.\n",
    "\n",
    "Each review was labeled as Positive (1), Neutral (0), or Negative (-1).  We kept the result of this analysis in the [_sentiment_analysis.csv_](https://drive.google.com/file/d/1attiqoXvTSruf6DDXodvU7jFvLKZCOHr/view?usp=sharing) file.\n",
    "\n",
    "We then compared the modelsâ performances based on exact similarity and similarity within a range of Â±1. The latter is particularly relevant because it allows us to identify models that may produce false positives or false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, exact_similarity_columns, plus_minus_1_similarity_columns] = compute_similarity_scores(sentiment_analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to plot the result in the following bar plots for the exact and Â±1 similarity scores across different pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_similarities(df, exact_similarity_columns, plus_minus_1_similarity_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, looking at the bar plots, the BERT and Distlilbert, the Google Cloud NLP and GPT-4o mini models, as well as the BERT anf Google Cloud NLP models show the highest similarity for exact sentiment, exceeding 65%.\n",
    "\n",
    "Additionally, the Google Cloud NLP and GPT-4o mini models have nearly 100% similarity when considering a margin, followed closely by the BERT-based model and GPT-4o mini, and then by the DistilBERT model and GPT-4o mini.\n",
    "\n",
    "Given that GPT-4o mini, the BERT model and Google Cloud NLP are very similar in the second graph, that GPT-4o mini is falling behind in the exact sentiment comparison and the DistilBERT model is behind the other models in both comparisons, we should use either Google Cloud NLP or the BERT-based model. Even though Google Cloud NLP has a slightly better similarity in both comparisons, it also an expensive api for large datasets. We will therefore use the BERT-based model for the final sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Comparing the distribution with the other attributes\n",
    "\n",
    "Now that we have a sentiment score for the comments with a textual review we can compare the distribution with the other grades. We want to see if it adds information, or if it is related with another attribute. For this we first need to recompute the grades provided by the sentiment analysis as the score is between -1 and 1. We can then filter the remaining beers and perform our statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recomputes the grades\n",
    "ratings_df_with_sent = recompute_grade(ratings_with_sent_df, min_grade_value = 1, max_grade_value = 5, attributes_of_interest = ['appearance', 'aroma', 'palate', 'taste', 'overall', 'rating','sentiment_bert'])\n",
    "#Filters the remaining beers, breweries, users and ratings\n",
    "beer_ratings_new_sent,breweries_df_new_sent, beers_df_new_sent, users_df_new_sent = filter_ratings_new(ratings_df_with_sent, beers_df, breweries_df,users_df, threshold=10, attributes=['appearance', 'aroma', 'palate', 'taste', 'overall','sentiment_bert'])\n",
    "#Computes the variance of the various attributes of interest\n",
    "attributes_variance_sent = compute_variance_per_attribute(beer_ratings_new_sent, ['appearance', 'aroma', 'palate', 'taste','overall','sentiment_bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that by keeping only comment with a textual description, we keep 19.22% of the initial beers. However we still keep 56.10% of the comments.\n",
    "\n",
    "As mentioned previously we want to test if the distribution of the variance provided by the sentiment analysis is similar to the distribution of another attribute. Therefore, we decided to perform a T-Test statistic to evaluate this hypothesis. The T-Test is applied on the variance of each attribute. The H0 hypothesis is that the true mean of the variance of a given attribute between the different beers are equal. H1 is that the means are different.\n",
    "We decided to plot the results in a Heatmap where the value is the p-value of the statistical test between the variance of two attributes. The diagonal should be 1 as it is twice the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_statistic(attributes_variance_sent, attributes_of_interest = ['appearance', 'aroma', 'palate', 'taste','overall','sentiment_bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous plot we can reject the H0 every time with a significance level of 5%. This means that the sentiment analysis provide further information, than the attributes we already have. We now want to see how variance of the sentiment score is correlated with the other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = compute_correlation(attributes_variance_sent, ['sentiment_bert'])\n",
    "plot_correlation(correlations, 'Correlation of attributes variances with the variance of the sentiment analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we see that the sentiment score variance is positively correlated with all the other attributes. Every correlation is around 0.4, however the appearence is particularly low at around 0.31. This could tell us that the textual review does not provid einformation abou the appearence. However such hypothesis would need to be studied more carefully with keywords analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3: Clustering and comparison against the previous clusters\n",
    "\n",
    "In part 2, we clustered the beers using GMM on certain attributes. Now we can add the information provided by the sentiment analysis and compare the different clusters. We aim at evaluating the clusters choosing how we will define controversiality for the rest of our analysis. Now lets us first perform the GMM clustering, we decided to use three clusters since we already used 3 clusters for the previous clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JE DEMANDE QUAND MEME ON MET LA ELBOW METHOD AUSSI, OU LA JUSTIFICATION DITES AU DESSUS SUFFIT? @Jehan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sent = visualize_gmm_all_dimensions(attributes_variance_sent, n_components_gmm=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous part, we see with the plots what the different clusters look like across the various attributes. By using the definition of controversiality, which would be that a higher variance indicates controversiality, whereas a near 0 variance indicates universality. From this we determine that in the case of sentiment analysis, that the clusters are the following:\n",
    "- 0 : neutral\n",
    "- 1 : universal\n",
    "- 2 : controversial\n",
    "\n",
    "Note that for plotting purposes and for the GMM computation that the variance where scaled to have 0 mean and 1 of variance. Therefore the values appearing on the plot are not the real ones, just the scaled variances. As our variances are positive, we assume that the smallest \"scaled variances\" are the ones that where near 0, and the bigger ones are the ones with a higher variance. \n",
    "\n",
    "Now we want to compare the two clustering made and see if they differ or not. The comparison will be done on the beers which contain a textual review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_two_clustering(attributes_variance, attributes_variance_sent, labels, labels_sent, [2,0,1], [1,0,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that clusters do not match at all. For this we believe the cause is coming from the sentiment score. During the evaluation process of the model we saw that we had 66% of perfect accuracy if the texts are classified as 0,-1 or 1. Therefore it introduces a higher variance since we are not sure about the final score. We also see that number of beers per label differs from the initial clustering. In the end we would need real labels to have a sentiment score which would serve as ground truth and on which we could rely. For all this reason we decide to pursue the analysis of the next parts with the clusters discovered in part 2.\n",
    "\n",
    "In conclusion, sentiment analysis provides another attribute which is different from the other attributes filled in by the grader. However we need more accurate results or at least results on which we could rely more to be confident in using them for the next step of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
